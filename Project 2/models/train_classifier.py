import sys
import pandas as pd
from sqlalchemy import create_engine
import nltk
import re
import pickle
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
nltk.download('stopwords')

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.multioutput import MultiOutputClassifier
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import GridSearchCV


def load_data(database_filepath):    
    '''
    load_data:
    Load data form file DB and 
    '''
    # Load data from file DB
    engine = create_engine('sqlite:///' + database_filepath)
    df = pd.read_sql_table('disaster_response_data', engine)
    X = df['message']
    Y = df.iloc[:, 4:]
    category_names = list(df.columns[4:])
    
    return X, Y, category_names


def tokenize(text):
    '''
    tokenize:
    Handle data is of string type.
    INPUT:
    text: String
    RETURN:
    lemmed: Characters after processing.
    '''    
    # Normalize Text
    text = re.sub(r"[^a-zA-Z0-9]", ' ', text.lower())
    
    # Tokenize
    words = word_tokenize(text)
    
    # Remove Stopwords
    words = [w for w in words if w not in stopwords.words('english')]
    
    # Lemmatize
    lemmatizer = WordNetLemmatizer()
    lemmed = [lemmatizer.lemmatize(w, pos='n').strip() for w in words]
    lemmed = [lemmatizer.lemmatize(w, pos='v').strip() for w in lemmed]
    
    return lemmed


def build_model():
    '''
    build_model:
    This machine pipeline should take in the message column as 
    input and output classification results on the other 36
    categories in the dataset.
    INPUT: none
    RETURN: 
    cv: Parameter better of model.
    '''
    # Build a machine learning pipeline.
    pipeline = Pipeline([
        ('vect', CountVectorizer(tokenizer=tokenize)),
        ('tfidf', TfidfTransformer()),
        ('clf', MultiOutputClassifier(RandomForestClassifier()))
    ])
    
    # Use grid search to find better parameters.
    parameters = {
        'tfidf__use_idf': (True, False)
    }    
    cv = GridSearchCV(pipeline, parameters, verbose=1)
    
    return cv


def evaluate_model(model, X_test, Y_test, category_names):    
    '''
    evaluate_model:
    Report the f1 score, precision and recall for each output category of the dataset.
    INPUT:
    model: Output classification results on the other 36 categories in the dataset.
    X_test: Data message
    Y_test: Data categories 
    category_names: name column category
    '''
    # Report the f1 score.
    Y_pred = model.predict(X_test)
    # Calculate the accuracy for each of them.
    for i in range(len(category_names)):
        print("Category:", category_names[i], "\n", classification_report(Y_test.iloc[:, i].values, Y_pred[:, i]))
        print('Accuracy of %25s: %.2f' % (category_names[i], accuracy_score(Y_test.iloc[:, i].values, Y_pred[:, i])))


def save_model(model, model_filepath):
    '''
    Saving model into file classifier.pkl.
    '''
    pickle.dump(model, open(model_filepath, "wb"))


def main():
    if len(sys.argv) == 3:
        database_filepath, model_filepath = sys.argv[1:]
        print('Loading data...\n    DATABASE: {}'.format(database_filepath))
        X, Y, category_names = load_data(database_filepath)
        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)
        
        print('Building model...')
        model = build_model()
        
        print('Training model...')
        model.fit(X_train, Y_train)
        
        print('Evaluating model...')
        evaluate_model(model, X_test, Y_test, category_names)

        print('Saving model...\n    MODEL: {}'.format(model_filepath))
        save_model(model, model_filepath)

        print('Trained model saved!')

    else:
        print('Please provide the filepath of the disaster messages database '\
              'as the first argument and the filepath of the pickle file to '\
              'save the model to as the second argument. \n\nExample: python '\
              'train_classifier.py ../data/DisasterResponse.db classifier.pkl')


if __name__ == '__main__':
    main()
